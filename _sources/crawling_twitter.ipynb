{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3AGRGGgpafc21g/N4SV87"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##Crawl_twitter"],"metadata":{"id":"Kv-peVF2KKtw"}},{"cell_type":"code","source":["!pip install snscrape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01hLM7bQiANK","executionInfo":{"status":"ok","timestamp":1676891665158,"user_tz":-420,"elapsed":4547,"user":{"displayName":"20-041 CHENDY TRI WARDANI","userId":"00046774953941734865"}},"outputId":"268fbd80-080e-4e0a-ac32-05bb7778c8e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: snscrape in /usr/local/lib/python3.8/dist-packages (0.5.0.20230113)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.6.3)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from snscrape) (2022.7.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.9.2)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from snscrape) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from snscrape) (3.9.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.24.3)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.7.1)\n"]}]},{"cell_type":"code","source":["import snscrape.modules.twitter as sntwitter\n","import pandas as pd"],"metadata":{"id":"AUiWK2JqhuzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# maxTweets = 100\n","query = \"(aniesbaswedan) until:2023-02-20 since:2022-01-01\"\n","tweets = []\n","limit = 500\n","\n","\n","tweets_list1 = []\n","for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n","  if len(tweets)==limit:\n","    break\n","  else:\n","    tweets.append([tweet.date, tweet.user.username, tweet.content, tweet.user.location])\n","\n","df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet', 'lokasi'])\n","print(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-O1hhhGiJWs","executionInfo":{"status":"ok","timestamp":1676901429519,"user_tz":-420,"elapsed":19575,"user":{"displayName":"20-041 CHENDY TRI WARDANI","userId":"00046774953941734865"}},"outputId":"b8aa3218-e79c-4e31-b8e4-79b34b972e04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-17-b1f7a6b0ed6b>:12: FutureWarning: content is deprecated, use rawContent instead\n","  tweets.append([tweet.date, tweet.user.username, tweet.content, tweet.user.location])\n"]},{"output_type":"stream","name":"stdout","text":["                         Date             User  \\\n","0   2023-02-19 23:59:48+00:00        sukurpldy   \n","1   2023-02-19 23:59:04+00:00        congciret   \n","2   2023-02-19 23:58:38+00:00  sabar_setiabudi   \n","3   2023-02-19 23:58:30+00:00         chaduque   \n","4   2023-02-19 23:58:16+00:00  sangkalacambang   \n","..                        ...              ...   \n","495 2023-02-19 17:10:37+00:00    budifebricool   \n","496 2023-02-19 17:10:32+00:00    AncolMangkrak   \n","497 2023-02-19 17:10:21+00:00       Sigmahalim   \n","498 2023-02-19 17:10:11+00:00       MajinSamri   \n","499 2023-02-19 17:09:55+00:00       BanGor9302   \n","\n","                                                 Tweet  \\\n","0    @NasDem @aniesbaswedan Sepertinya akan banyak ...   \n","1    @NasDem @adityawilly @aniesbaswedan Ga percaya...   \n","2    @IrsanHarahap7 @MulyadiSaediman @aniesbaswedan...   \n","3    @aniesbaswedan Nggak percuma waktu di DKI kasi...   \n","4    @ewondoanx99 @aniesbaswedan Anjing anjing AS d...   \n","..                                                 ...   \n","495  @marlina_idha @aniesbaswedan Moment haram jadi...   \n","496  @Sigmahalim @aniesbaswedan @NasDem @PDemokrat ...   \n","497  @damarjati_dimas @TeguhSudarisman @babesije @a...   \n","498  @dimaspram96 @Sukardi2017 @aniesbaswedan Ikuta...   \n","499  @ronindrawan @ch_chotimah2 @aniesbaswedan Junj...   \n","\n","                          lokasi  \n","0              Tangerang, Banten  \n","1                                 \n","2                       Bandung   \n","3                                 \n","4             Makasar, Indonesia  \n","..                           ...  \n","495                               \n","496  Kalimantan Timur, Indonesia  \n","497                               \n","498                               \n","499                 Bumi اللّهُ   \n","\n","[500 rows x 4 columns]\n"]}]},{"cell_type":"code","source":["df.to_csv('crawl_twitter2.csv')"],"metadata":{"id":"RomekpMDzioz","executionInfo":{"status":"ok","timestamp":1676901932231,"user_tz":-420,"elapsed":626,"user":{"displayName":"20-041 CHENDY TRI WARDANI","userId":"00046774953941734865"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUGOGO1XgRuR","executionInfo":{"status":"ok","timestamp":1676896739116,"user_tz":-420,"elapsed":24488,"user":{"displayName":"20-041 CHENDY TRI WARDANI","userId":"00046774953941734865"}},"outputId":"8a02f4c3-3120-45cc-f58b-b17a0e02cc1d"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-468ce372716a>:29: FutureWarning: content is deprecated, use rawContent instead\n","  tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n","<ipython-input-15-468ce372716a>:52: FutureWarning: content is deprecated, use rawContent instead\n","  tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n"]}],"source":["# Script Author: Martin Beck\n","# Medium Article Follow-Along: https://medium.com/better-programming/how-to-scrape-tweets-with-snscrape-90124ed006af\n","\n","# Pip install the command below if you don't have the development version of snscrape \n","# !pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n","\n","# Run the below command if you don't already have Pandas\n","# !pip install pandas\n","\n","# Imports\n","import snscrape.modules.twitter as sntwitter\n","import pandas as pd\n","\n","# Below are two ways of scraping using the Python Wrapper.\n","# Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n","# then output two different csv files.\n","\n","# Query by username\n","# Setting variables to be used below\n","maxTweets = 100\n","\n","# Creating list to append tweet data to\n","tweets_list1 = []\n","\n","# Using TwitterSearchScraper to scrape data \n","for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:jack').get_items()):\n","    if i>maxTweets:\n","        break\n","    tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n","\n","# Creating a dataframe from the tweets list above\n","tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n","\n","# Display first 5 entries from dataframe\n","# tweets_df1.head()\n","\n","# Export dataframe into a CSV\n","tweets_df1.to_csv('user-tweets.csv', sep=',', index=False)\n","\n","\n","# Query by text search\n","# Setting variables to be used below\n","maxTweets = 500\n","\n","# Creating list to append tweet data to\n","tweets_list2 = []\n","\n","# Using TwitterSearchScraper to scrape data and append tweets to list\n","for i,tweet in enumerate(sntwitter.TwitterSearchScraper('its the elephant since:2020-06-01 until:2020-07-31').get_items()):\n","    if i>maxTweets:\n","        break\n","    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n","\n","# Creating a dataframe from the tweets list above\n","tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n","\n","# Display first 5 entries from dataframe\n","tweets_df2.head()\n","\n","# Export dataframe into a CSV\n","tweets_df2.to_csv('D:\\\\Chendy Tri Wardani\\\\Kuliah\\\\Semester 6\\\\prosaindata\\\\twitter.csv', sep=',', index=False)"]}]}